#!/usr/bin/env python3
"""
å®Œæ•´æ–‡æ¡£æ³¨é‡Šæå–è„šæœ¬

ä½¿ç”¨æ‰¹é‡å¤„ç†æ–¹æ³•æå–å®Œæ•´çš„è´¢åŠ¡æŠ¥è¡¨æ³¨é‡Šç« èŠ‚
"""

import sys
import os
import json
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.parsers.batch_notes_extractor import BatchNotesExtractor
from src.parsers.config_loader import load_llm_config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)


def extract_full_document(
    pdf_path: str,
    start_page: int,
    end_page: int,
    output_path: str,
    batch_size: int = 5
):
    """
    æå–å®Œæ•´æ–‡æ¡£çš„æ³¨é‡Šå†…å®¹

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        start_page: èµ·å§‹é¡µç 
        end_page: ç»“æŸé¡µç 
        output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤5é¡µï¼‰
    """
    logger.info("=" * 70)
    logger.info("å¼€å§‹æå–å®Œæ•´æ–‡æ¡£æ³¨é‡Š")
    logger.info("=" * 70)
    logger.info(f"PDFæ–‡ä»¶: {pdf_path}")
    logger.info(f"é¡µç èŒƒå›´: {start_page}-{end_page} (å…±{end_page - start_page + 1}é¡µ)")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}é¡µ/æ‰¹æ¬¡")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    logger.info("")

    # åŠ è½½é…ç½®
    config = load_llm_config()
    logger.info(f"ä½¿ç”¨é…ç½®: provider={config['provider']}, model={config['model']}")
    logger.info("")

    # åˆ›å»ºæå–å™¨
    extractor = BatchNotesExtractor(
        provider=config['provider'],
        model=config['model'],
        api_key=config.get('api_key'),
        base_url=config.get('base_url')
    )

    # é¢„ä¼°æ—¶é—´
    total_pages = end_page - start_page + 1
    num_batches = (total_pages + batch_size - 1) // batch_size
    estimated_time = num_batches * 2.3  # æ¯æ‰¹æ¬¡çº¦2.3åˆ†é’Ÿ

    logger.info(f"é¢„ä¼°ä¿¡æ¯:")
    logger.info(f"  â€¢ æ‰¹æ¬¡æ•°: {num_batches}")
    logger.info(f"  â€¢ é¢„ä¼°è€—æ—¶: {estimated_time:.1f}åˆ†é’Ÿ")
    logger.info(f"  â€¢ LLMè°ƒç”¨æ¬¡æ•°: {num_batches}æ¬¡")
    logger.info("")

    # å¼€å§‹æå–
    start_time = datetime.now()
    logger.info("å¼€å§‹æå–...")
    logger.info("")

    try:
        result = extractor.extract_notes_batch(
            pdf_path=pdf_path,
            start_page=start_page,
            end_page=end_page,
            batch_size=batch_size
        )

        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        # ä¿å­˜ç»“æœ
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        # æ‰“å°ç»“æœ
        logger.info("")
        logger.info("=" * 70)
        logger.info("æå–å®Œæˆï¼")
        logger.info("=" * 70)
        logger.info(f"æˆåŠŸ: {result['success']}")
        logger.info(f"æ€»é¡µæ•°: {result['total_pages']}")
        logger.info(f"æå–æ ‡é¢˜æ•°: {result['total_notes']}")
        logger.info(f"å®é™…è€—æ—¶: {elapsed:.2f}ç§’ ({elapsed/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"å¹³å‡é€Ÿåº¦: {elapsed/total_pages:.2f}ç§’/é¡µ")

        if result['errors']:
            logger.warning(f"é”™è¯¯æ•°: {len(result['errors'])}")
            for error in result['errors']:
                logger.warning(f"  â€¢ {error}")

        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        logger.info("")

        # ç»Ÿè®¡ä¿¡æ¯
        notes_with_content = sum(1 for n in result['notes'] if n.get('content'))
        notes_with_tables = sum(1 for n in result['notes'] if n.get('has_table'))
        total_tables = sum(n.get('content', {}).get('table_count', 0) for n in result['notes'])

        logger.info("å†…å®¹ç»Ÿè®¡:")
        logger.info(f"  â€¢ åŒ…å«å†…å®¹çš„æ ‡é¢˜: {notes_with_content}/{result['total_notes']} ({notes_with_content/result['total_notes']*100:.1f}%)")
        logger.info(f"  â€¢ åŒ…å«è¡¨æ ¼çš„æ ‡é¢˜: {notes_with_tables}/{result['total_notes']} ({notes_with_tables/result['total_notes']*100:.1f}%)")
        logger.info(f"  â€¢ æ€»è¡¨æ ¼æ•°: {total_tables}")
        logger.info("")

        # æ˜¾ç¤ºå‰10ä¸ªæ ‡é¢˜
        logger.info("å‰10ä¸ªæ ‡é¢˜:")
        for i, note in enumerate(result['notes'][:10], 1):
            has_content = "âœ“" if note.get('content') else "âœ—"
            has_table = "ğŸ“Š" if note.get('has_table') else ""
            logger.info(f"  {i:2d}. [{note['page_num']:3d}é¡µ] {note['full_title']:<40s} {has_content} {has_table}")

        if result['total_notes'] > 10:
            logger.info(f"  ... è¿˜æœ‰ {result['total_notes'] - 10} ä¸ªæ ‡é¢˜")

        logger.info("")
        logger.info("=" * 70)

        return result

    except Exception as e:
        logger.error(f"æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='æå–å®Œæ•´æ–‡æ¡£çš„è´¢åŠ¡æŠ¥è¡¨æ³¨é‡Š')
    parser.add_argument('pdf_path', help='PDFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('start_page', type=int, help='èµ·å§‹é¡µç ')
    parser.add_argument('end_page', type=int, help='ç»“æŸé¡µç ')
    parser.add_argument('-o', '--output', help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šoutput/notes_full.jsonï¼‰')
    parser.add_argument('-b', '--batch-size', type=int, default=5, help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š5ï¼‰')

    args = parser.parse_args()

    # è®¾ç½®é»˜è®¤è¾“å‡ºè·¯å¾„
    if args.output is None:
        output_dir = project_root / 'output'
        output_dir.mkdir(exist_ok=True)
        args.output = str(output_dir / 'notes_full.json')

    # æ‰§è¡Œæå–
    extract_full_document(
        pdf_path=args.pdf_path,
        start_page=args.start_page,
        end_page=args.end_page,
        output_path=args.output,
        batch_size=args.batch_size
    )


if __name__ == '__main__':
    main()
